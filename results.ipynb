{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Created temporary evaluation directory: ./eval_images\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "from torch_fidelity import calculate_metrics\n",
    "import lpips\n",
    "\n",
    "from models import Generator\n",
    "from encoder import FaceNetEncoder\n",
    "from dataLoaders import get_dataloaders\n",
    "\n",
    "CHECKPOINT_EPOCH = 20\n",
    "CHECKPOINT_DIR = \"./checkpoints_widerface\"\n",
    "GENERATOR_PATH = os.path.join(CHECKPOINT_DIR, f\"generator_epoch_{CHECKPOINT_EPOCH}.pth\")\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 128\n",
    "NOISE_DIM = 100\n",
    "EMBEDDING_DIM = 512\n",
    "\n",
    "EVAL_DIR = \"./eval_images\"\n",
    "REAL_IMG_DIR = os.path.join(EVAL_DIR, \"real\")\n",
    "FAKE_IMG_DIR = os.path.join(EVAL_DIR, \"fake\")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if os.path.exists(EVAL_DIR):\n",
    "    shutil.rmtree(EVAL_DIR)\n",
    "os.makedirs(REAL_IMG_DIR, exist_ok=True)\n",
    "os.makedirs(FAKE_IMG_DIR, exist_ok=True)\n",
    "print(f\"Created temporary evaluation directory: {EVAL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Generator weights from epoch 20 loaded successfully.\n",
      "\n",
      "Loading test dataset...\n",
      "Loading training data from: ./data/train\n",
      "Warning: Skipping corrupted or invalid file: ./data/test/.DS_Store\n",
      "Loading test data from: ./data/test\n",
      "Warning: Skipping corrupted or invalid file: ./data/test/.DS_Store\n",
      "\n",
      "DataLoaders created successfully!\n",
      "Number of training images: 10000\n",
      "Number of testing images: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading models...\")\n",
    "encoder = FaceNetEncoder(device=device)\n",
    "encoder.eval()\n",
    "\n",
    "generator = Generator(noise_dim=NOISE_DIM, embedding_dim=EMBEDDING_DIM).to(device)\n",
    "try:\n",
    "    generator.load_state_dict(torch.load(GENERATOR_PATH, map_location=device))\n",
    "    print(f\"Generator weights from epoch {CHECKPOINT_EPOCH} loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Checkpoint file not found at '{GENERATOR_PATH}'\")\n",
    "    raise\n",
    "\n",
    "generator.eval()\n",
    "print(\"\\nLoading test dataset...\")\n",
    "_, test_loader = get_dataloaders(data_root='.', batch_size=BATCH_SIZE, image_size=IMAGE_SIZE)\n",
    "\n",
    "if not test_loader:\n",
    "    raise RuntimeError(\"Could not create test dataloader.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating fake images for the entire test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Images: 100%|██████████| 16/16 [00:09<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 500 real and fake images to './eval_images' for metric calculation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "all_real_images = []\n",
    "all_fake_images = []\n",
    "img_idx = 0\n",
    "\n",
    "print(\"\\nGenerating fake images for the entire test set...\")\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Generating Images\"):\n",
    "        real_images_batch = batch[0].to(device)\n",
    "        \n",
    "        # Get embeddings and create noise\n",
    "        real_embeddings = encoder(real_images_batch)\n",
    "        noise = torch.randn(real_images_batch.size(0), NOISE_DIM, device=device)\n",
    "        \n",
    "        # Generate fake images\n",
    "        fake_images_batch = generator(noise, real_embeddings)\n",
    "        \n",
    "        # Store tensors for LPIPS calculation later\n",
    "        all_real_images.append(real_images_batch.cpu())\n",
    "        all_fake_images.append(fake_images_batch.cpu())\n",
    "\n",
    "        # Save individual images for FID calculation\n",
    "        for i in range(real_images_batch.size(0)):\n",
    "            # Un-normalize before saving to disk\n",
    "            real_img_unnorm = real_images_batch[i] * 0.5 + 0.5\n",
    "            fake_img_unnorm = fake_images_batch[i] * 0.5 + 0.5\n",
    "            \n",
    "            save_image(real_img_unnorm, os.path.join(REAL_IMG_DIR, f\"{img_idx}.png\"))\n",
    "            save_image(fake_img_unnorm, os.path.join(FAKE_IMG_DIR, f\"{img_idx}.png\"))\n",
    "            img_idx += 1\n",
    "\n",
    "print(f\"\\nSaved {img_idx} real and fake images to '{EVAL_DIR}' for metric calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calculating LPIPS Score ---\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /Users/0xr4plh/Documents/Machine Learning/Generative Training 3/invideo/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating LPIPS: 100%|██████████| 16/16 [00:00<00:00, 37.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LPIPS Result ---\n",
      "Average LPIPS Score: 0.5103\n",
      "(Lower is better)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Calculating LPIPS Score ---\")\n",
    "# Use the 'alex' network, which is standard for this metric\n",
    "lpips_model = lpips.LPIPS(net='alex').to(device)\n",
    "\n",
    "total_lpips_distance = 0\n",
    "num_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for real_batch, fake_batch in tqdm(zip(all_real_images, all_fake_images), total=len(all_real_images), desc=\"Calculating LPIPS\"):\n",
    "        real_batch = real_batch.to(device)\n",
    "        fake_batch = fake_batch.to(device)\n",
    "        \n",
    "        distance = lpips_model(real_batch, fake_batch)\n",
    "        total_lpips_distance += distance.sum().item()\n",
    "        num_batches += 1\n",
    "\n",
    "# Average the score over all images\n",
    "average_lpips = total_lpips_distance / (num_batches * BATCH_SIZE)\n",
    "\n",
    "print(\"\\n--- LPIPS Result ---\")\n",
    "print(f\"Average LPIPS Score: {average_lpips:.4f}\")\n",
    "print(\"(Lower is better)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "invideo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
